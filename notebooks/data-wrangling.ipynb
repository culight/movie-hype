{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the relevant paths and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the auto-reload for the cells with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# get source and data directories\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "\n",
    "# set the source data directory as the current path\n",
    "src_data_dir = os.path.join(src_dir, 'data')\n",
    "sys.path.append(src_data_dir)\n",
    "\n",
    "interim_data_dir = os.path.join(data_dir, 'interim')\n",
    "processed_data_dir = os.path.join(data_dir, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the web scraping modules\n",
    "%aimport feedback\n",
    "import feedback as fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%aimport clean\n",
    "import clean as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the existing csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#audit movie financial csv file\n",
    "cl.clean_imdb_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape movie data and put into separate csv files\n",
    "I segment the movies this way in case I encounter some kind of error for a particular year, I only disregard that year so as to not lose all of the data from the other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting year: 2000\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2000.csv...\n",
      " done!\n",
      "0.050003015995\n",
      "finished year: 2000\n",
      "starting year: 2001\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2001.csv...\n",
      " done!\n",
      "0.0656933347384\n",
      "finished year: 2001\n",
      "starting year: 2002\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2002.csv...\n",
      " done!\n",
      "0.524883568287\n",
      "finished year: 2002\n",
      "starting year: 2003\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2003.csv...\n",
      " done!\n",
      "1.84589506785\n",
      "finished year: 2003\n",
      "starting year: 2004\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2004.csv...\n",
      " done!\n",
      "1.73590893348\n",
      "finished year: 2004\n",
      "starting year: 2005\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2005.csv...\n",
      "non-ascii characters found\n",
      " done!\n",
      "1.92024291754\n",
      "finished year: 2005\n",
      "starting year: 2006\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2006.csv...\n",
      " done!\n",
      "1.72474666834\n",
      "finished year: 2006\n",
      "starting year: 2007\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2007.csv...\n",
      " done!\n",
      "1.66737925212\n",
      "finished year: 2007\n",
      "starting year: 2008\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2008.csv...\n",
      "non-ascii characters found\n",
      " done!\n",
      "1.73101145029\n",
      "finished year: 2008\n",
      "starting year: 2009\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2009.csv...\n",
      "non-ascii characters found\n",
      " done!\n",
      "2.3036904494\n",
      "finished year: 2009\n",
      "starting year: 2010\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2010.csv...\n",
      " done!\n",
      "2.79076476892\n",
      "finished year: 2010\n",
      "starting year: 2011\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n",
      " done!\n",
      " creating /Users/dmoton/projects/data_projects/movie-hype/notebooks/../data/interim/2011.csv...\n",
      "non-ascii characters found\n",
      " done!\n",
      "3.44062364896\n",
      "finished year: 2011\n",
      "starting year: 2012\n",
      " getting movie urls... \n",
      " done!\n",
      " scraping data from movie website...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', BadStatusLine(\"''\",))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9f32e9ca58fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# scrape the website for the movie data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmovie_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# add the interest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmoton/projects/data_projects/movie-hype/notebooks/../src/data/feedback.pyc\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(movie_urls)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovie_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmovie_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mmovie_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dmoton/projects/data_projects/movie-hype/notebooks/../src/data/feedback.pyc\u001b[0m in \u001b[0;36mget_html\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/DAND/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', BadStatusLine(\"''\",))"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# get a list of movies for a particular year\n",
    "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', \\\n",
    "         '2007', '2008', '2009', '2010', '2011', '2012', '2013', \\\n",
    "         '2014', '2015', '2016']\n",
    "\n",
    "for year in years:\n",
    "    print('starting year: ' + str(year))\n",
    "    movie_urls = []\n",
    "    movie_data = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # get a list of movie urls for each desired year\n",
    "    movie_urls = fb.get_movie_urls(year)\n",
    "    \n",
    "    # scrape the website for the movie data\n",
    "    movie_data = fb.scrape(movie_urls)\n",
    "    \n",
    "    # add the interest score\n",
    "    avg_rating = np.array([m['rating'] for m in movie_data]).mean()\n",
    "    avg_num_votes = np.array([m['total_votes'] for m in movie_data]).mean()\n",
    "    for movie in movie_data:\n",
    "        movie['interest_score'] = ((avg_rating*avg_num_votes) + (movie['total_votes']*movie['rating']))/(avg_num_votes + movie['total_votes'])\n",
    "    \n",
    "    # create a csv for the given year\n",
    "    fb.convert_to_csv(movie_data, interim_data_dir, str(year))\n",
    "    \n",
    "    end_time = time.time() - start_time\n",
    "    print(end_time/60)\n",
    "    print('finished year: ' + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the csv files into one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# join the interest data into a dataframe\n",
    "mi = pd.DataFrame([])\n",
    "\n",
    "for year in years:\n",
    "    year_csv = os.path.join(interim_data_dir, year + '.csv')\n",
    "    year_df = pd.read_csv(year_csv, skiprows=0)\n",
    "    mi = mi.append(year_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the interest score\n",
    "ratings = list(mi['rating'])\n",
    "num_votes = list(mi['total_votes'])\n",
    "\n",
    "avg_rating = np.array(ratings).mean()\n",
    "avg_num_votes = np.array(num_votes).mean()\n",
    "mi['interest_score'] = ((avg_rating*avg_num_votes) + (mi['total_votes']*mi['rating']))/(avg_num_votes + mi['total_votes'])\n",
    "\n",
    "output = os.path.join(processed_data_dir, 'movie_interest.csv')\n",
    "mi.to_csv(output) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
